{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import __future__, sys\n",
    "\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Process\n",
    "file=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\bioArabic.xml'\n",
    "import xml.etree.cElementTree as ET\n",
    "tree = ET.parse(file)\n",
    "root = tree.getroot()\n",
    "for elem in root.getiterator():\n",
    "    print (elem.tag, elem.attrib)\n",
    "n=0\n",
    "for group in root.iter('sentence'):\n",
    "          print(group.text)\n",
    "          n+=1\n",
    "\n",
    "c=0\n",
    "for neighbor in root.iter('cue'):\n",
    "    print(neighbor.text)\n",
    "    c+=1\n",
    "\n",
    "xx=0\n",
    "for neighbor in root.iter('cue'):\n",
    "    if neighbor.text != None:\n",
    "        print(neighbor.text)\n",
    "        xx+=1\n",
    "        \n",
    "x=0\n",
    "for neighbor in root.iter('xcope'):\n",
    "    if neighbor.text != None:\n",
    "        print(neighbor.text)\n",
    "        x+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def dbdepc(depen):\n",
    "    li=[]\n",
    "    import os\n",
    "    import sys\n",
    "    par=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\parsed.txt'\n",
    "    depen=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\dep.txt'\n",
    "    def my_datablock_iter(file):\n",
    "        \n",
    "\n",
    "        for line in file:\n",
    "        # find ID\n",
    "            if \"root\" in line:\n",
    "            # build a list of lines until FIN is seen\n",
    "                wanted = [line.strip()]\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line == \" \":\n",
    "                        break\n",
    "                    wanted.append(line)\n",
    "            # hand block back to user\n",
    "                yield wanted\n",
    "\n",
    "    with open(depen,'r', encoding='utf-8') as fp:\n",
    "        for datablock in  my_datablock_iter(fp):\n",
    "            li.append(datablock)\n",
    "    li=str(li).replace('[','').replace('\"','')\n",
    "    li= str(li).replace(']','').replace('\"','')\n",
    "    return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def dbdepc1(par):\n",
    "    li=[]\n",
    "    import os\n",
    "    import sys\n",
    "    par=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\parsed.txt'\n",
    "    depen=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\dep.txt'\n",
    "    def my_datablock_iter(file):\n",
    "        \n",
    "\n",
    "        for line in file:\n",
    "        # find ID\n",
    "            if \"(ROOT\" in line:\n",
    "            # build a list of lines until FIN is seen\n",
    "                wanted = [line.strip()]\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line == \" \":\n",
    "                        break\n",
    "                    wanted.append(line)\n",
    "            # hand block back to user\n",
    "                yield wanted\n",
    "\n",
    "    with open(par,'r', encoding='utf-8') as fp:\n",
    "        for datablock in  my_datablock_iter(fp):\n",
    "            li.append(datablock)\n",
    "    li=str(li).replace('[','').replace('\"','')\n",
    "    li= str(li).replace(']','').replace('\"','')\n",
    "    li=str(li).replace(\"'\", \"\")\n",
    "    li=str(li).replace(\", , , \", \"\")\n",
    "    return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "from nltk.tree import ParentedTree\n",
    "\n",
    "\n",
    "def get_lca_length(location1, location2):\n",
    "    i = 0\n",
    "    while i < len(location1) and i < len(location2) and location1[i] == location2[i]:\n",
    "        i+=1\n",
    "    return i\n",
    "\n",
    "def get_labels_from_lca(ptree, lca_len, location):\n",
    "    labels = []\n",
    "    for i in range(lca_len, len(location)):\n",
    "        labels.append(ptree[location[:i]].label())\n",
    "    return labels\n",
    "\n",
    "def findPath(ptree, text1, text2):\n",
    "    leaf_values = ptree.leaves()\n",
    "    leaf_index1 = leaf_values.index(text1)\n",
    "    leaf_index2 = leaf_values.index(text2)\n",
    "\n",
    "    location1 = ptree.leaf_treeposition(leaf_index1)\n",
    "    location2 = ptree.leaf_treeposition(leaf_index2)\n",
    "\n",
    "    #find length of least common ancestor (lca)\n",
    "    lca_len = get_lca_length(location1, location2)\n",
    "\n",
    "    #find path from the node1 to lca\n",
    "\n",
    "    labels1 = get_labels_from_lca(ptree, lca_len, location1)\n",
    "    #ignore the first element, because it will be counted in the second part of the path\n",
    "    result = labels1[1:]\n",
    "    #inverse, because we want to go from the node to least common ancestor\n",
    "    result = result[::-1]\n",
    "\n",
    "    #add path from lca to node2\n",
    "    result = result + get_labels_from_lca(ptree, lca_len, location2)\n",
    "    return result\n",
    "\n",
    "# ptree = ParentedTree.fromstring(' (ROOT(S(NP (DT The) (NNS doctors)) \\\n",
    "#         (VP (VBP warn)(SBAR (IN that)(S(NP (NN smoking))(VP (MD may)(VP (VB harm)(NP (PRP$ our) (NNS lungs)))))))(. .)))')\n",
    "# # print(ptree.pprint())\n",
    "\n",
    "list1=[]\n",
    "\n",
    "\n",
    "\n",
    "for neighbor in root.iter('cue'):\n",
    "    list1.append(neighbor.text)\n",
    "def listToStringWithoutBrackets(list1):\n",
    "    return str(list1).replace('[','').replace(']','')\n",
    "liss=listToStringWithoutBrackets(list1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations=['acl','advcl','advmod','amod','appos','aux','case','cc','ccomp','clf','compound','conj','cop','csubj','dep','det','discourse','dislocated','expl',\n",
    "'fixed','flat','goeswith','iobj','list','mark','nmod','nsubj','nummod','obj','obl','orphan','parataxis','punct','reparandum','root','vocative','xcomp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isWordIn(word, string):\n",
    "    return not(string.find(word) == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple usage\n",
    "\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Process\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP(r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\stanford-corenlp-full-2018-10-05\\stanford-corenlp-full-2018-10-05')\n",
    "propst={'annotators': 'tokenize,pos,parse,depparse','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "propsp={'annotators': 'pos','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "propsconst={'annotators': 'parse','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "propsdep={'annotators': 'depparse','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "\n",
    "\n",
    "import csv\n",
    "names_file = open('Arabic2019.csv', 'w+',encoding=\"utf-8\")\n",
    "writer = csv.writer(names_file)\n",
    "writer.writerow(['cue','word','fp','d','scope','label'])\n",
    "# for n in df['sentence/0/__text']:\n",
    "parsed=nlp.annotate(sentence, properties=propsconst)\n",
    "dep=nlp.annotate(sentence, properties=propsdep)\n",
    "par=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\parsed.txt'\n",
    "depen=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\dep.txt'\n",
    "\n",
    "with open(depen, encoding='utf-8', mode='w') as file1:\n",
    "    file1.truncate(0)\n",
    "    file1.write(dep)\n",
    "#     print(dep)\n",
    "    \n",
    "the_datablock=dbdepc(depen)\n",
    "# print(the_datablock)\n",
    "ww=the_datablock.split(\"', '\")\n",
    "# for rr in ww:\n",
    "#     print(rr)\n",
    "\n",
    "writer.writerow(ww)\n",
    "  \n",
    "with open(par, encoding='utf-8', mode='w') as file1:\n",
    "    file1.truncate(0)\n",
    "    file1.write(parsed)\n",
    "#     print(dep)\n",
    "\n",
    "www=dbdepc1(par)\n",
    "# for rr in ww:\n",
    "#     print(rr)\n",
    "# writer.writerow(www)\n",
    "# ptree = ParentedTree.fromstring(www)\n",
    "ptree = ParentedTree.fromstring(www)\n",
    "cue=''\n",
    "d=''\n",
    "id_s=0\n",
    "scope=[]\n",
    "label =False\n",
    "for word in nlp.word_tokenize(sentence):\n",
    "    if word in list1:\n",
    "        cue=word\n",
    "        for word in nlp.word_tokenize(sentence):\n",
    "            fp=findPath(ptree,cue,word)\n",
    "            for rr in ww:\n",
    "                for dd in relations:\n",
    "                    if isWordIn(d,rr):\n",
    "                        d=dd\n",
    "        for p in root.iter():\n",
    "            if p is not None:\n",
    "                pa =  p.find('sentence')\n",
    "                if pa is not None:\n",
    "                    for v in pa.find('xcope'):\n",
    "                        if v is not None:\n",
    "                            vi = v.find('xcope')\n",
    "                            if word in vi:\n",
    "                                scope.append(word)\n",
    "                                label=True\n",
    "                                \n",
    "                        \n",
    "            writer.writerow([cue,word,fp,d,scope,label])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "from multiprocessing import Process\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP(r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\stanford-corenlp-full-2018-10-05\\stanford-corenlp-full-2018-10-05')\n",
    "propst={'annotators': 'tokenize,pos,parse,depparse','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "propsp={'annotators': 'pos','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "propsconst={'annotators': 'parse','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "propsdep={'annotators': 'depparse','pipelineLanguage':'ar','outputFormat':'text'}\n",
    "\n",
    "for group in root.iter('sentence'):\n",
    "    import csv\n",
    "    names_file = open('Arabic2019.csv', 'w+',encoding=\"utf-8\")\n",
    "    writer = csv.writer(names_file)\n",
    "    writer.writerow(['cue','word','fp','d','scope','label'])\n",
    "# for n in df['sentence/0/__text']:\n",
    "    parsed=nlp.annotate(sentence, properties=propsconst)\n",
    "    dep=nlp.annotate(sentence, properties=propsdep)\n",
    "    par=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\parsed.txt'\n",
    "    depen=r'C:\\Users\\Fatima\\Desktop\\UNcertainity-arabic\\dep.txt'\n",
    "\n",
    "    with open(depen, encoding='utf-8', mode='w') as file1:\n",
    "        file1.truncate(0)\n",
    "        file1.write(dep)\n",
    "#     print(dep)\n",
    "    \n",
    "    the_datablock=dbdepc(depen)\n",
    "# print(the_datablock)\n",
    "   ww=the_datablock.split(\"', '\")\n",
    "# for rr in ww:\n",
    "#     print(rr)\n",
    "\n",
    "    writer.writerow(ww)\n",
    "  \n",
    "    with open(par, encoding='utf-8', mode='w') as file1:\n",
    "        file1.truncate(0)\n",
    "        file1.write(parsed)\n",
    "#     print(dep)\n",
    "\n",
    "    www=dbdepc1(par)\n",
    "# for rr in ww:\n",
    "#     print(rr)\n",
    "# writer.writerow(www)\n",
    "# ptree = ParentedTree.fromstring(www)\n",
    "    ptree = ParentedTree.fromstring(www)\n",
    "    cue=''\n",
    "    d=''\n",
    "    id_s=0\n",
    "    scope=[]\n",
    "    label =False\n",
    "    for word in nlp.word_tokenize(sentence):\n",
    "        id_s+=1\n",
    "        if word in list1:\n",
    "            cue=word\n",
    "            for word in nlp.word_tokenize(sentence):\n",
    "                fp=findPath(ptree,cue,word)\n",
    "                for rr in ww:\n",
    "                    for dd in relations:\n",
    "                        if isWordIn(d,rr):\n",
    "                            d=dd\n",
    "            for p in root.iter():\n",
    "                if p is not None:\n",
    "                    pa =  p.find('sentence')\n",
    "                    if pa is not None:\n",
    "                        for v in pa.find('xcope'):\n",
    "                            if v is not None:\n",
    "                                vi = v.find('xcope')\n",
    "                                if word in vi:\n",
    "                                    scope.append(word)\n",
    "                                    label=True\n",
    "                                \n",
    "                        \n",
    "            writer.writerow([id_s,cue,word,fp,d,scope,label])\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'C:\\Users\\Fatima\\Arabic2019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# datanew = []\n",
    "# rownum=0\n",
    "# with open(r'C:\\Users\\Fatima\\Arabic2019.csv',encoding=\"utf8\") as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for row in reader:\n",
    "#         rownum +=1\n",
    "#         datanew.append(row)\n",
    "# #     print ('row',datanew)\n",
    "#     print(rownum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocabulary_size = 10000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['id_s'])\n",
    "sequences = tokenizer.texts_to_sequences(df['id_s'])\n",
    "data = pad_sequences(sequences, maxlen=30)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(200, 100, input_length=30))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# encoder = LabelBinarizer()\n",
    "# transfomed_label = encoder.fit_transform([\"ccue\"])\n",
    "# print(transfomed_label)\n",
    "model_lstm.fit(data, df['label'],validation_split=0.4, epochs=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    vocabulary_size=30\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=30))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "    return model_conv\n",
    "model_conv = create_conv_model()\n",
    "model_conv.fit(data, df['label'],validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.predict(\n",
    "    data, \n",
    "    batch_size=32, \n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.predict_classes(data,batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(400, 16))\n",
    "conv1 = Conv1D(filters=32,\n",
    "               kernel_size=8,\n",
    "               strides=1,\n",
    "               activation='relu',\n",
    "               padding='same')(input_layer)\n",
    "lstm1 = LSTM(32, return_sequences=True)(conv1)\n",
    "output_layer = Dense(1, activation='sigmoid')(lstm1)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pooling and merging\n",
    "from keras.layers import Input, Dense, LSTM, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(400, 16))\n",
    "conv1 = Conv1D(filters=32,\n",
    "               kernel_size=8,\n",
    "               strides=1,\n",
    "               activation='relu')(input_layer)\n",
    "pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "lstm1 = LSTM(32)(pool1)\n",
    "output_layer = Dense(400, activation='sigmoid')(lstm1)\n",
    "model_merging= Model(inputs=input_layer, outputs=output_layer)\n",
    "model_merging.fit(data, df['label'],validation_split=0.4, epochs = 3)\n",
    "\n",
    "model_merging.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Convolution2D\n",
    "from keras.models import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras_attention_block import *\n",
    "\n",
    "INPUT_DIM = 32\n",
    "TIME_STEPS = 20\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "\n",
    "inputs = Input(shape=(TIME_STEPS, INPUT_DIM))\n",
    "attention_mul =  SelfAttention1DLayer(similarity=\"linear\",dropout_rate=0.2)(inputs)#MyLayer((20,32))(inputs)#\n",
    "lstm_units = 32\n",
    "#attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "m = Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())\n",
    "\n",
    "\n",
    "m.fit(data, df['label'],validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, LSTM, MaxPooling1D, Conv1D\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout,LSTM, Bidirectional\n",
    "# modelBi = Sequential()\n",
    "# modelBi.add(Bidirectional(LSTM(9, return_sequences=True),\n",
    "#                         input_shape=(10, 9)))\n",
    "# modelBi.add(Bidirectional(LSTM(9)))\n",
    "# modelBi.add(Dense(10))\n",
    "# modelBi.add(Activation('softmax'))\n",
    "# modelBi.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "# modelBi.fit(data,df['label'] , epochs=1, batch_size=100 )\n",
    "max_features=10\n",
    "maxlen=9\n",
    "modelB = Sequential()\n",
    "\n",
    "modelB.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "modelB.add(Conv1D(64, 3, activation='relu', input_shape=(10, 9)))\n",
    "modelB.add(Conv1D(64, 3, activation='relu'))\n",
    "modelB.add(MaxPooling1D(3))\n",
    "modelB.add(Bidirectional(LSTM(64)))\n",
    "# modelB.add(Conv1D(32, 3, activation='relu'))\n",
    "# modelB.add(Conv1D(32, 3, activation='relu'))\n",
    "# modelB.add(Conv1D(64, 2, activation='relu', input_shape=(10, 9)))\n",
    "modelB.add(Dropout(0.5))\n",
    "modelB.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "modelB.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "# modelB.fit(data,df['label'] , epochs=3, batch_size=100 )\n",
    "modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_vonv = model_conv.predict(data)\n",
    "y_predlstm = model_lstm.predict(data)\n",
    "y_predmerg=  model_merging.predict(data)\n",
    "y_predatten=m..predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true =label\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print(classification_report(df['label'], y_pred_vonv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true =label\n",
    "# >>> y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print(classification_report(df['label'], y_predlstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true =label\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print(classification_report(df['label'], y_pred_vonv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true =label\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print(classification_report(df['label'], y_predmerg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true =label\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print(classification_report(df['label'], y_predatten))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
